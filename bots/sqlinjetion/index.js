require("events").EventEmitter.setMaxListeners = 0;
require("events").EventEmitter.defaultMaxListeners = 0;
var request = require("request");
var Crawler = require("js-crawler");
var unique = require("array-unique");
const global = require("../../src/functionsglobal");
const config = require("../../config.json");
const log = global.log;
var path = require("path");

CapturaNovo();

//Função de captura na para verificação de framework
async function CapturaNovo() {
    //Busca na API um array de dominios para verificação
    var dominio = await GET();
    //Verifica se a api retornou valores
    if (dominio.rows[0] != undefined) {
        //Loop para verificação de cada um dos dominios retornados pela API
        await asyncForEach(dominio.rows, async (value) => {
            //Chama a função de Verificação de Framework
            await Verifica(value.domain, value.id);
        });
        //Caso não tenha retornado dados da API
    } else {
        //Exibe no terminal a espera de um minuto para nova verificação
        console.log("Aguarda 1 Minuto");
        //Aguarda 1 minuto
        await global.sleep(60000);
    }
    //Chama a mesma função novamente para loop infinito
    CapturaNovo();
}

//função para loop async
async function asyncForEach(array, callback) {
    for (let index = 0; index < array.length; index++) {
        await callback(array[index], index, array);
    }
}

async function Verifica(url, id) {
    log("URL: www." + url);
    //Array de links para remover repetidos
    var mylinks = [];
    //Array de Emails para remover repetidos
    var myemails = [];
    //Inicia uma promise para aguardar todo processo
    return new Promise(async function (resolve, reject) {
        //Monta o retorno com id e identidade
        var retorno = { id: id };
        //Retorna a API o resultado do framework encontrado no dominio
        POST(retorno);

        //Inicio do Crawler
        new Crawler()
            .configure({
                shouldCrawl: function (urlx) {
                    if (
                        path.extname(urlx) == ".jpg" ||
                        path.extname(urlx) == ".jpeg" ||
                        path.extname(urlx) == ".pdf" ||
                        path.extname(urlx) == ".css" ||
                        path.extname(urlx) == ".svg" ||
                        path.extname(urlx) == ".map" ||
                        path.extname(urlx) == ".png" ||
                        path.extname(urlx) == ".doc" ||
                        path.extname(urlx) == ".docx" ||
                        path.extname(urlx) == ".xls" ||
                        path.extname(urlx) == ".zip" ||
                        path.extname(urlx) == ".rar" ||
                        path.extname(urlx) == ".tar" ||
                        path.extname(urlx) == ".js" ||
                        path.extname(urlx) == ".htm" ||
                        path.extname(urlx) == ".html" ||
                        urlx.endsWith("/blog") ||
                        urlx.endsWith("/blog/")
                    ) {
                        return false;
                    } else if (
                        urlx.startsWith("http://" + url) ||
                        urlx.startsWith("https://" + url) ||
                        urlx.startsWith("http://www." + url) ||
                        urlx.startsWith("https://www." + url)
                    ) {
                        return true;
                    } else {
                        return false;
                    }
                },
                ignoreRelative: false,
                depth: 2,
                maxRequestsPerSecond: 10,
                maxConcurrentRequests: 5,
                userAgent:
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36",
            })
            .crawl(
                "http://www." + url,
                function onSuccess(page) {
                    //Passa o valor da url para tratamento
                    var urlx = page.url;

                    //Remove das string da url todos os amp;
                    urlx = urlx.replace(/amp;/g, "");

                    try {
                        //Decode da URL
                        urlx = decodeURIComponent(urlx);
                    } catch (error) {
                        log(urlx + ": " + error, 1);
                    }

                    //verifica pertence ao dominio
                    if (
                        urlx.startsWith("http://" + url) ||
                        urlx.startsWith("https://" + url) ||
                        urlx.startsWith("http://www." + url) ||
                        urlx.startsWith("https://www." + url)
                    ) {
                        //captura emails da página
                        var emailsArray = page.content.match(
                            /(?:[a-zA-Z0-9_.-]+(?:\.[a-zA-Z0-9_.-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]*[a-zA-Z0-9])?\.)+[a-zA-Z0-9](?:[a-zA-Z0-9-]*[a-zA-Z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-zA-Z0-9-]*[a-zA-Z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])/g
                        );
                        //Verifica se retornou email
                        if (emailsArray) {
                            //Retira do array email repetidos
                            emailsArray = unique(emailsArray);
                            //verifica todos emails do array
                            for (var i = 0; i < emailsArray.length; i++) {
                                //verifica se a url atual já foi salva
                                if (myemails.indexOf(emailsArray[i]) == -1) {
                                    //Adiciona o email atual a lista de emails já obtidas
                                    myemails.push(emailsArray[i]);
                                    if (
                                        emailsArray[i].endsWith(".gif") ==
                                            false ||
                                        emailsArray[i].endsWith(".jpg") ==
                                            false ||
                                        emailsArray[i].endsWith(".png") == false
                                    ) {
                                        log(emailsArray[i], 3);
                                        //salva emails capturados com a página onde foram localizados
                                        global.Savelogs(
                                            emailsArray[i],
                                            "SQLiMails"
                                        );
                                    }
                                }
                            }
                        }
                        log(urlx, 1);
                        //verifica se a url é passivel de sql injetion
                        if (
                            /\?([a-zA-Z]\w+|[a-zA-Z])=([a-zA-Z0-9])*/g.test(
                                urlx
                            ) &&
                            urlx.indexOf(";O=") == -1
                        ) {
                            //separa a url para verificar repetição
                            var arr = urlx.split("=");

                            //verifica se a url atual já foi salva
                            if (mylinks.indexOf(arr[0]) == -1) {
                                //Exibe no terminal a url capturada
                                log(urlx, 3);
                                //Adiciona a url atual a lista de url já obtidas
                                mylinks.push(arr[0]);
                                //salva no log a url obtida
                                global.Savelogs(urlx, "SQLiLinks_ID");
                            }
                        } else if (/\/([0-9]\w+|[0-9])$/g.test(urlx)) {
                            //separa a url para verificar repetição
                            var arr = urlx.split("/");
                            //verifica se a url atual já foi salva
                            if (mylinks.indexOf(arr[arr.length - 2]) == -1) {
                                //Exibe no terminal a url capturada
                                log(urlx, 3);
                                //Adiciona a url atual a lista de url já obtidas
                                mylinks.push(arr[arr.length - 2]);
                                //salva no log a url obtida
                                global.Savelogs(urlx, "SQLiLinks_URL");
                            }
                        }
                    }
                },
                function (response) {
                    log(
                        "ERROR: " + response.url + " [" + response.status + "]",
                        2
                    );
                },
                function onAllFinished(crawledUrls) {
                    //Exibe no terminal o fim do crawling
                    log("All crawling finished");
                    log(
                        "Capturados: " +
                            mylinks.length +
                            " de [" +
                            crawledUrls.length +
                            "] URLS"
                    );

                    //Retorna a promise false
                    resolve(true);
                }
            );
    });
}

async function GET() {
    return new Promise(async function (resolve, reject) {
        var options = {
            uri: config.Host_Externo + "/sqliget/br",
            method: "GET",
        };

        request(options, async function (error, response, body) {
            resolve(JSON.parse(body));
        });
    });
}

function POST(dados) {
    var options = {
        uri: config.Host_Externo + "/sqlipost",
        headers: {
            authorization: config.token,
        },
        method: "POST",
        json: { id: dados.id },
    };

    request(options, function (error, response, body) {
        //console.log(body);
    });
}
